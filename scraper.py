#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JW.ORG Scraper COMPLETO ‚Üí Todoist
Monitora TODAS as se√ß√µes do jw.org e cria tarefas automaticamente
Vers√£o: 3.0 - Completa
"""

import os
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import hashlib

# ============================================================
# CONFIGURA√á√ïES
# ============================================================

TODOIST_TOKEN = os.environ['TODOIST_TOKEN']
STATE_FILE = 'state.json'

# Todas as se√ß√µes para monitorar
SECTIONS = {
    'homepage': {
        'url': 'https://www.jw.org/pt/',
        'name': 'üè† P√°gina Principal',
        'priority': 2,
        'label': 'destaque'
    },
    'sentinela': {
        'url': 'https://www.jw.org/pt/biblioteca/revistas/w/',
        'name': 'üìñ A Sentinela',
        'priority': 2,
        'label': 'sentinela'
    },
    'despertai': {
        'url': 'https://www.jw.org/pt/biblioteca/revistas/g/',
        'name': 'üìñ Despertai',
        'priority': 2,
        'label': 'despertai'
    },
    'videos': {
        'url': 'https://www.jw.org/pt/biblioteca/videos/',
        'name': 'üé• V√≠deos JW',
        'priority': 3,
        'label': 'videos'
    },
    'noticias': {
        'url': 'https://www.jw.org/pt/noticias/',
        'name': 'üì∞ Not√≠cias',
        'priority': 3,
        'label': 'noticias'
    },
    'apostila': {
        'url': 'https://www.jw.org/pt/biblioteca/jw-apostila-do-mes/',
        'name': 'üìã Apostila da Reuni√£o',
        'priority': 1,
        'label': 'apostila'
    },
    'livros': {
        'url': 'https://www.jw.org/pt/biblioteca/livros/',
        'name': 'üìö Publica√ß√µes',
        'priority': 3,
        'label': 'publicacoes'
    }
}

# ============================================================
# FUN√á√ïES
# ============================================================

def load_state():
    """Carrega estado anterior (conte√∫dos j√° vistos)"""
    try:
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {}

def save_state(state):
    """Salva estado atual"""
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, f, indent=2, ensure_ascii=False)

def scrape_section(url, section_name):
    """Extrai conte√∫dos de uma se√ß√£o do jw.org"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8'
    }

    try:
        print(f"   üì° Acessando {section_name}...")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # Procurar links de conte√∫do
        all_links = soup.find_all('a', href=True)

        items = []
        seen_urls = set()

        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # Filtrar conte√∫do v√°lido
            if not text or len(text) < 15:  # T√≠tulo muito curto
                continue

            # Montar URL completa
            if href.startswith('/'):
                full_url = f"https://www.jw.org{href}"
            elif href.startswith('http'):
                full_url = href
            else:
                continue

            # Apenas conte√∫do do jw.org
            if 'jw.org' not in full_url:
                continue

            # Excluir links de navega√ß√£o/menu
            exclude_keywords = [
                '/contato', '/ajuda', '/idiomas', '/busca', '/sobre',
                '/termos-de-uso', '/politica-de-privacidade', '/copyright',
                '/ajustes', '/login', '/conta', 'javascript:', '#'
            ]

            if any(keyword in href.lower() for keyword in exclude_keywords):
                continue

            # Evitar duplicatas
            if full_url in seen_urls:
                continue

            seen_urls.add(full_url)

            # Criar hash √∫nico
            item_hash = hashlib.md5(full_url.encode()).hexdigest()

            items.append({
                'title': text[:100],  # Limitar tamanho
                'url': full_url,
                'hash': item_hash
            })

        print(f"   ‚úÖ {len(items)} itens encontrados")
        return items

    except requests.Timeout:
        print(f"   ‚ö†Ô∏è Timeout ao acessar {section_name}")
        return []
    except Exception as e:
        print(f"   ‚ùå Erro ao acessar {section_name}: {str(e)[:50]}")
        return []

def create_todoist_task(title, url, section_name, priority, label):
    """Cria tarefa no Todoist"""
    headers = {
        "Authorization": f"Bearer {TODOIST_TOKEN}",
        "Content-Type": "application/json"
    }

    # Limpar t√≠tulo (remover caracteres problem√°ticos)
    clean_title = title.replace('\n', ' ').replace('\t', ' ').strip()
    clean_title = ' '.join(clean_title.split())  # Remover espa√ßos m√∫ltiplos

    payload = {
        "content": f"{section_name}: {clean_title}",
        "description": f"üîó Link: {url}\n\nüìÖ Adicionado em {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}\nüè∑Ô∏è Se√ß√£o: {section_name}",
        "priority": priority,
        "labels": ["jw.org", label]
    }

    try:
        response = requests.post(
            "https://api.todoist.com/rest/v2/tasks",
            headers=headers,
            json=payload,
            timeout=10
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"      ‚ùå Erro ao criar tarefa: {str(e)[:50]}")
        return None

def main():
    """Fun√ß√£o principal"""
    print("=" * 70)
    print("üîç JW.ORG SCRAPER COMPLETO ‚Üí TODOIST")
    print(f"üìÖ Executado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
    print("=" * 70)
    print()

    # Carregar estado
    state = load_state()

    total_new = 0

    # Processar cada se√ß√£o
    for section_key, section_config in SECTIONS.items():
        print(f"üîé {section_config['name']}")

        # Obter estado anterior desta se√ß√£o
        section_state = state.get(section_key, {'seen_hashes': []})
        seen_hashes = section_state.get('seen_hashes', [])

        # Fazer scraping
        items = scrape_section(section_config['url'], section_config['name'])

        new_count = 0

        # Verificar novos itens
        for item in items:
            if item['hash'] not in seen_hashes:
                print(f"   üÜï Novo: {item['title'][:55]}...")

                # Criar tarefa
                task = create_todoist_task(
                    item['title'],
                    item['url'],
                    section_config['name'],
                    section_config['priority'],
                    section_config['label']
                )

                if task:
                    print(f"      ‚úÖ Tarefa criada (ID: {task['id']})")
                    new_count += 1
                    total_new += 1
                    seen_hashes.append(item['hash'])

        if new_count == 0:
            print(f"   ‚ÑπÔ∏è Nenhum conte√∫do novo")

        # Limitar hist√≥rico (√∫ltimos 100 itens por se√ß√£o)
        seen_hashes = seen_hashes[-100:]

        # Atualizar estado da se√ß√£o
        section_state['seen_hashes'] = seen_hashes
        section_state['last_check'] = datetime.now().isoformat()
        state[section_key] = section_state

        print()

    # Salvar estado geral
    state['last_run'] = datetime.now().isoformat()
    save_state(state)

    print("=" * 70)
    print(f"‚úÖ FINALIZADO: {total_new} nova(s) tarefa(s) criada(s)")
    print("=" * 70)

if __name__ == "__main__":
    main()
